# LLaMA3-8B Finetuning with Unsloth for Materials Composition Classification

[![Author](https://img.shields.io/badge/Author-harshu0117-blue)](https://github.com/harshu0117)
[![Model](https://img.shields.io/badge/Model-LLaMA3--8B-green)](https://huggingface.co/meta-llama)
[![Framework](https://img.shields.io/badge/Unsloth-Finetuning%20Made%20Easy-orange)](https://github.com/unslothai/unsloth)

> **Repo Link**: [github.com/harshu0117/llama3_8B_finetune_unsloth](https://github.com/harshu0117/llama3_8B_finetune_unsloth)

---

## ğŸ“Œ Project Objective

This project demonstrates **instruction-tuned fine-tuning** of **Meta-LLaMA 3.1 8B** using the **Unsloth** framework, applied to a **materials informatics task**:  
> **Predict whether a given chemical composition is a metal.**

The goal is to explore the **capability of LLMs in materials property prediction** by fine-tuning on structured instruction-response data and benchmarking against the raw base model.

---

## ğŸ› ï¸ Methodology

- Used `train.json` and `test.json` files with instruction-input-output triples.
- Formatted data using **Alpaca-style prompt templates**.
- Finetuned using `SFTTrainer` from `trl` with 4-bit quantized weights.
- Performed inference with both:
  - ğŸ”¹ Base raw LLaMA-3.1 8B model
  - ğŸ”¸ Finetuned LLaMA-3.1 8B model
- Evaluated using accuracy, precision, recall, and F1-score.
- Visualized predictions using pie charts.

---

## ğŸ” Prompt Format (Alpaca Style)

```txt
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
Given composition, is it metal? ->

### Input:
Ba5Sb3

### Response:
Yes, Ba5Sb3 is metal.
````

---

## ğŸ“Š Evaluation Results

| Model        | Accuracy   | Precision  | Recall     | F1 Score   |
| ------------ | ---------- | ---------- | ---------- | ---------- |
| ğŸ”¹ Raw Model | 0.5294     | 0.5294     | 1.0000     | 0.6923     |
| ğŸ”¸ Finetuned | **0.9020** | **0.9231** | **0.8889** | **0.9057** |

### ğŸ“ˆ Finetuned Accuracy Pie Chart

![Finetuned Accuracy](llma_raw_accuracy_pie.png)

### ğŸ“ˆ Raw Model Accuracy Pie Chart

![Raw Accuracy](accuracy_pie_chart.png)

---

## ğŸ§ª Significance in Materials Domain

Large Language Models (LLMs) like LLaMA3 can be fine-tuned to act as **domain-specific property predictors** for materials science. This can enable:

* ğŸš€ **Rapid screening** of novel compositions.
* ğŸ” **Text-to-property inference** without DFT or experiments.
* ğŸ¤– **Foundations for Materials Copilots** â€” LLMs assisting scientists with structure-property insights.

**Use Cases**:

* Predictive pre-screening for alloys, oxides, semiconductors.
* Integration into materials design pipelines.
* Enhancing materials databases with natural language querying.

---

## ğŸ“‚ Repo Structure

```
llama3_8B_finetune_unsloth/
â”‚
â”œâ”€â”€ train.json               # Training data
â”œâ”€â”€ test.json                # Evaluation data
â”œâ”€â”€ llma_finetuned.json      # Output from fine-tuned model
â”œâ”€â”€ llma_raw.json            # Output from base model
â”œâ”€â”€ accuracy_pie_chart.png   # Finetuned accuracy plot
â”œâ”€â”€ llma_raw_accuracy_pie.png # Raw model accuracy plot
â”œâ”€â”€ finetune_script.py       # Script to train using SFTTrainer
â”œâ”€â”€ inference_script.py      # Script to run inference
â””â”€â”€ README.md                # This file
```

---

## ğŸš€ Future Work

* Fine-tune on more detailed tasks: phase prediction, band gap estimation, formation energy.
* Add multi-modal support (composition + structure).
* Integrate with materials graph neural networks (e.g., MatDeepLearn, CGCNN).
* Create web demo for real-time inference.

---

## ğŸ¤ Acknowledgements

* [Unsloth](https://github.com/unslothai/unsloth)
* [Meta AI](https://ai.meta.com/)
* [HuggingFace Transformers](https://huggingface.co/docs/transformers)
* Community contributors in materials AI.

---

ğŸ“¬ For questions or contributions, feel free to open an issue or reach out at:
ğŸ‘‰ [github.com/harshu0117](https://github.com/harshu0117)

```

---

Let me know if you'd like:
- Badge-style metrics from Hugging Face Spaces.
- GitHub Actions CI/CD for auto-training.
- Colab notebook version of training/inference.
```
